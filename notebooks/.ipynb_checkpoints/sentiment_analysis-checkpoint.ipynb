{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89ecb22e-feea-4293-91ba-8ec9b6057bd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m custom_keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFDA approval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice target\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearnings report\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Identify keywords (unigrams and bigrams) with custom keywords\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m keywords \u001b[38;5;241m=\u001b[39m identify_keywords(headlines, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, custom_keywords\u001b[38;5;241m=\u001b[39mcustom_keywords)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 Keywords/Phrases:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword, count \u001b[38;5;129;01min\u001b[39;00m keywords:\n",
      "Cell \u001b[1;32mIn[18], line 93\u001b[0m, in \u001b[0;36midentify_keywords\u001b[1;34m(headlines, n, custom_keywords)\u001b[0m\n\u001b[0;32m     91\u001b[0m keywords \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m headline \u001b[38;5;129;01min\u001b[39;00m headlines:\n\u001b[1;32m---> 93\u001b[0m   preprocessed_text \u001b[38;5;241m=\u001b[39m preprocess_text(headline)\n\u001b[0;32m     94\u001b[0m   words \u001b[38;5;241m=\u001b[39m preprocessed_text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import re  # Import for regular expressions\n",
    "\n",
    "def clean_text(text):\n",
    "  \"\"\"\n",
    "  Cleans text by performing lowercasing, removing punctuation, and removing stop words.\n",
    "\n",
    "  Args:\n",
    "      text (str): The text to be cleaned.\n",
    "\n",
    "  Returns:\n",
    "      str: The cleaned text.\n",
    "  \"\"\"\n",
    "  text = text.lower()  # Lowercase\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "  stop_words = stopwords.words('english')\n",
    "  text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "  return text\n",
    "\n",
    "def analyze_sentiment(headline):\n",
    "  \"\"\"\n",
    "  Analyzes the sentiment (positive, negative, neutral) of a headline.\n",
    "\n",
    "  Args:\n",
    "      headline (str): The headline to be analyzed.\n",
    "\n",
    "  Returns:\n",
    "      str: The sentiment category (positive, negative, neutral).\n",
    "  \"\"\"\n",
    "  analyzer = SentimentIntensityAnalyzer()\n",
    "  sentiment = analyzer.polarity_scores(clean_text(headline))\n",
    "  if sentiment['compound'] > 0.05:\n",
    "    return 'positive'\n",
    "  elif sentiment['compound'] < -0.05:\n",
    "    return 'negative'\n",
    "  else:\n",
    "    return 'neutral'\n",
    "\n",
    "def load_and_clean_data(data_file):\n",
    "  \"\"\"\n",
    "  Loads financial data from a CSV file, performs cleaning steps, and performs sentiment analysis on headlines.\n",
    "\n",
    "  Args:\n",
    "      data_file (str): Path to the CSV file containing financial data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: Cleaned DataFrame containing financial data with sentiment analysis results.\n",
    "  \"\"\"\n",
    "\n",
    "  data = pd.read_csv(data_file)\n",
    "\n",
    "  # Handle missing values\n",
    "  data.dropna(subset=['stock', 'date'], inplace=True)\n",
    "\n",
    "  # Handle duplicates (keep only the most recent per stock)\n",
    "  data.sort_values(by=['date'], ascending=False, inplace=True)  # Sort by date (descending)\n",
    "  data.drop_duplicates(subset='stock', keep='last', inplace=True)\n",
    "\n",
    "  # Attempt date conversion\n",
    "  try:\n",
    "    # Adjust format if needed (e.g., '%Y-%m-%d %H:%M:%S')\n",
    "    data['date'] = pd.to_datetime(data['date'], errors='coerce', format='mixed', utc=True)  # Optional: utc=True\n",
    "  except ValueError:\n",
    "    print(\"Error: Date format conversion failed with specified format. Trying 'mixed' format...\")\n",
    "    try:\n",
    "      data['date'] = pd.to_datetime(data['date'], errors='coerce', format='mixed')\n",
    "      print(\"Successfully parsed dates using 'mixed' format.\")\n",
    "    except:\n",
    "      print(\"Failed to convert all dates. Daily frequency analysis might be inaccurate.\")\n",
    "\n",
    "  # Add sentiment column\n",
    "  data['sentiment'] = data['headline'].apply(analyze_sentiment)\n",
    "\n",
    "  return data\n",
    "def identify_keywords(headline, n=1, custom_keywords=[]):\n",
    "  \"\"\"\n",
    "  Identifies common keywords and phrases in headlines.\n",
    "\n",
    "  Args:\n",
    "      headlines (list): List of headline strings.\n",
    "      n (int, optional): The length of n-grams to consider (default: 1 for unigrams).\n",
    "      custom_keywords (list, optional): A list of custom keywords to target (default: []).\n",
    "\n",
    "  Returns:\n",
    "      dict: A dictionary containing identified keywords and their frequencies.\n",
    "  \"\"\"\n",
    "  keywords = Counter()\n",
    "  for headline in headlines:\n",
    "    preprocessed_text = preprocess_text(headline)\n",
    "    words = preprocessed_text.split()\n",
    "    for i in range(n):\n",
    "      for j in range(len(words) - n + 1):\n",
    "        phrase = ' '.join(words[j:j+n])\n",
    "        keywords[phrase] += 1\n",
    "    # Check for custom keywords\n",
    "    for keyword in custom_keywords:\n",
    "      if keyword.lower() in preprocessed_text:\n",
    "        keywords[keyword] += 1\n",
    "  return keywords.most_common(10)  # Return top 10 most frequent keywords/phrases\n",
    "\n",
    "# Sample data (replace with your actual headlines)\n",
    "headlines = [\n",
    "  \"Apple's Stock Price Soars After Strong Earnings Report\",\n",
    "  \"FDA Approves New Drug for Cancer Treatment\",\n",
    "  \"Analyst Raises Price Target for Amazon\",\n",
    "  \"Tech Sector Expected to See Growth in Q3\",\n",
    "  \"Tesla Announces Plans for New Gigafactory\",\n",
    "]\n",
    "\n",
    "# Custom keywords for specific events\n",
    "custom_keywords = [\"FDA approval\", \"price target\", \"earnings report\"]\n",
    "\n",
    "# Identify keywords (unigrams and bigrams) with custom keywords\n",
    "keywords = identify_keywords(headlines, n=2, custom_keywords=custom_keywords)\n",
    "\n",
    "print(\"Top 10 Keywords/Phrases:\")\n",
    "for keyword, count in keywords:\n",
    "  print(f\"{keyword}: {count}\")\n",
    "\n",
    "\n",
    "# Load and clean data\n",
    "data = load_and_clean_data(\"../data/raw_analyst_ratings.csv\")\n",
    "# Access the 'date' column\n",
    "dates = data['date']\n",
    "\n",
    "# Access the 'sentiment' column\n",
    "sentiment = data['sentiment']\n",
    "sentiment_value_counts = sentiment.value_counts() \n",
    "\n",
    "# Access multiple columns at once (comma-separated)\n",
    "specific_data = data[['date', 'headline','sentiment']]\n",
    "print(sentiment)\n",
    "print(dates)\n",
    "print(specific_data)\n",
    "print(sentiment_value_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6ac1c-ff04-4653-9d95-2be2b61fdc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
